import os
import subprocess
import aiohttp
from bs4 import BeautifulSoup, SoupStrainer
from urllib.parse import urljoin, urlparse
import sys
import orjson
import asyncio
import aiofiles
from datetime import datetime
import typer
from typing import List, Optional


app = typer.Typer()


async def load_visited_urls(visited_urls_file: str):
    if os.path.exists(visited_urls_file):
        try:
            async with aiofiles.open(visited_urls_file, "r", encoding="utf-8") as f:
                content = await f.read()
                data = orjson.loads(content)
                visited_urls = set(data.get("visited_urls", []))
                print(f"ğŸ“ ì´ì „ ë°©ë¬¸ ê¸°ë¡ì„ ë¡œë“œí–ˆìŠµë‹ˆë‹¤: {len(visited_urls)}ê°œ URL")
                return visited_urls
        except (orjson.JSONDecodeError, IOError) as e:
            print(f"âš ï¸ ë°©ë¬¸ ê¸°ë¡ íŒŒì¼ì„ ì½ëŠ” ì¤‘ ì˜¤ë¥˜ ë°œìƒ: {e}")
            print("   ìƒˆë¡œìš´ ë°©ë¬¸ ê¸°ë¡ì„ ì‹œì‘í•©ë‹ˆë‹¤.")

    return set()


async def save_visited_urls(visited_urls: set, visited_urls_file: str):
    try:
        data = {
            "visited_urls": list(visited_urls),
            "last_updated": datetime.now().isoformat(),
        }
        async with aiofiles.open(visited_urls_file, "wb") as f:
            await f.write(
                orjson.dumps(data, option=orjson.OPT_INDENT_2 | orjson.OPT_NON_STR_KEYS)
            )
    except IOError as e:
        print(f"âš ï¸ ë°©ë¬¸ ê¸°ë¡ ì €ì¥ ì¤‘ ì˜¤ë¥˜ ë°œìƒ: {e}")


def check_aria2c_installed():
    try:
        subprocess.run(["aria2c", "--version"], check=True, capture_output=True)
        print("âœ… 'aria2c'ê°€ ì„¤ì¹˜ë˜ì–´ ìˆìŠµë‹ˆë‹¤.")
        return True
    except (subprocess.CalledProcessError, FileNotFoundError):
        print("âŒ ì˜¤ë¥˜: 'aria2c'ê°€ ì„¤ì¹˜ë˜ì–´ ìˆì§€ ì•Šê±°ë‚˜ PATHì— ì—†ìŠµë‹ˆë‹¤.")
        print("ì´ ìŠ¤í¬ë¦½íŠ¸ë¥¼ ì‹¤í–‰í•˜ë ¤ë©´ aria2ë¥¼ ì„¤ì¹˜í•´ì£¼ì„¸ìš”.")
        print("  - Debian/Ubuntu: sudo apt-get install aria2")
        print("  - CentOS/RHEL: sudo yum install aria2")
        return False


async def download_file_with_aria2(
    file_url: str,
    local_dir: str,
    aria2_options: List[str],
    error_urls_file: str,
):
    print(f"ğŸ“¥ íŒŒì¼ ë‹¤ìš´ë¡œë“œ ì‹œë„: {file_url}")
    if file_url.endswith(".metalink"):
        return await download_file_with_aiohttp(file_url, local_dir, error_urls_file)
    command = ["aria2c", "-c", "--dir", local_dir, *aria2_options, file_url]

    try:
        process = await asyncio.create_subprocess_exec(
            *command, stdout=asyncio.subprocess.PIPE, stderr=asyncio.subprocess.PIPE
        )
        stdout, stderr = await process.communicate()

        stdout_text = stdout.decode("utf-8", errors="ignore") if stdout else ""
        stderr_text = stderr.decode("utf-8", errors="ignore") if stderr else ""

        if process.returncode == 0:
            if "download completed" in stdout_text.lower():
                print(f"âœ… ë‹¤ìš´ë¡œë“œ ì™„ë£Œ: {file_url}")
                return True
            elif "already been completed" in stdout_text.lower():
                print(f"ğŸ‘ ê±´ë„ˆë›°ê¸° (ì´ë¯¸ ì™„ë£Œë¨): {os.path.basename(file_url)}")
                return True
            else:
                print(stdout_text)
                return True
        else:
            print(f"âš ï¸ aria2c ì‹¤í–‰ ì¤‘ ì˜¤ë¥˜ ë°œìƒ: {file_url}")
            print(f"   ì˜¤ë¥˜ ë©”ì‹œì§€: {stderr_text}")
            await save_error_url(file_url, error_urls_file)
            return False
    except Exception as e:
        print(f"ğŸš¨ ì˜ˆì™¸ ë°œìƒ: {e}")
        await save_error_url(file_url, error_urls_file)
        return False


async def download_file_with_aiohttp(
    file_url: str, local_dir: str, error_urls_file: str
):
    os.makedirs(local_dir, exist_ok=True)
    local_filename = os.path.join(local_dir, os.path.basename(file_url))
    try:
        timeout = aiohttp.ClientTimeout(total=60.0)
        async with aiohttp.ClientSession(timeout=timeout) as session:
            async with session.get(file_url) as response:
                response.raise_for_status()
                async with aiofiles.open(local_filename, "wb") as f:
                    async for chunk in response.content.iter_chunked(8192):
                        await f.write(chunk)
        print(f"âœ… .metalink íŒŒì¼ ë‹¤ìš´ë¡œë“œ ì™„ë£Œ: {local_filename}")
        return True
    except Exception as e:
        print(f"âŒ .metalink íŒŒì¼ ë‹¤ìš´ë¡œë“œ ì‹¤íŒ¨: {file_url}\n   ì˜¤ë¥˜: {e}")
        await save_error_url(file_url, error_urls_file)
        return False


async def save_error_url(error_url: str, error_urls_file: str):
    urls = []
    if os.path.exists(error_urls_file):
        try:
            async with aiofiles.open(error_urls_file, "r", encoding="utf-8") as f:
                content = await f.read()
                data = orjson.loads(content)
                urls = data.get("error_urls", [])
        except Exception:
            pass
    if error_url not in urls:
        urls.append(error_url)
    data = {
        "error_urls": urls,
        "last_updated": datetime.now().isoformat(),
    }
    try:
        async with aiofiles.open(error_urls_file, "wb") as f:
            await f.write(
                orjson.dumps(data, option=orjson.OPT_INDENT_2 | orjson.OPT_NON_STR_KEYS)
            )
    except Exception as e:
        print(f"âš ï¸ ì˜¤ë¥˜ URL ì €ì¥ ì¤‘ ë¬¸ì œ ë°œìƒ: {e}")


async def sync_mirror(
    current_url: str,
    local_path: str,
    visited_urls: set,
    base_sync_url: str,
    aria2_options: List[str],
    visited_urls_file: str,
    error_urls_file: str,
    visited_directories: Optional[set] = None,
):
    if visited_directories is None:
        visited_directories = set()

    normalized_url = current_url.rstrip("/")

    if normalized_url in visited_directories:
        print(f"â†ªï¸ ì´ë¯¸ ë°©ë¬¸í•œ ë””ë ‰í† ë¦¬ì…ë‹ˆë‹¤. ê±´ë„ˆëœë‹ˆë‹¤: {current_url}")
        return

    visited_directories.add(normalized_url)

    if normalized_url in visited_urls:
        print(f"â†ªï¸ ì´ë¯¸ ë°©ë¬¸í•œ URLì…ë‹ˆë‹¤. ê±´ë„ˆëœë‹ˆë‹¤: {current_url}")
        return

    print(f"\nğŸ“‚ ë””ë ‰í† ë¦¬ íƒìƒ‰ ì¤‘: {current_url}")

    os.makedirs(local_path, exist_ok=True)

    try:
        timeout = aiohttp.ClientTimeout(total=30.0)
        async with aiohttp.ClientSession(timeout=timeout) as session:
            async with session.get(current_url) as response:
                response.raise_for_status()
                response_text = await response.text()
    except aiohttp.ClientError as e:
        print(f"âŒ URLì— ì ‘ê·¼í•  ìˆ˜ ì—†ìŠµë‹ˆë‹¤: {current_url}\n   ì˜¤ë¥˜: {e}")
        return
    except Exception as e:
        print(f"âŒ HTTP ì˜¤ë¥˜ ë°œìƒ: {current_url}\n   ì˜¤ë¥˜: {e}")
        return

    strainer = SoupStrainer("a")
    soup = BeautifulSoup(response_text, "lxml", parse_only=strainer)

    links = soup.find_all("a")

    if not links:
        print(f"   -> í•´ë‹¹ ë””ë ‰í† ë¦¬ì—ì„œ ë§í¬ë¥¼ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤: {current_url}")
        return

    for link in links:
        href = link.get("href")

        if (
            not href
            or href.startswith("?")
            or href.startswith("../")
            or urlparse(href).scheme
        ):
            continue

        next_url = urljoin(current_url, href)

        if not next_url.startswith(base_sync_url):
            print(f"â†ªï¸ ë²”ìœ„ë¥¼ ë²—ì–´ë‚œ URLì…ë‹ˆë‹¤. ê±´ë„ˆëœë‹ˆë‹¤: {next_url}")
            continue

        if next_url == current_url:
            continue

        if href.endswith("/"):
            normalized_next_url = next_url.rstrip("/")
            if normalized_next_url not in visited_directories:
                next_local_path = os.path.join(local_path, href.strip("/"))
                await sync_mirror(
                    next_url,
                    next_local_path,
                    visited_urls,
                    base_sync_url,
                    aria2_options,
                    visited_urls_file,
                    error_urls_file,
                    visited_directories,
                )
            else:
                print(f"â†ªï¸ ì´ë¯¸ ë°©ë¬¸í•œ ë””ë ‰í† ë¦¬ì…ë‹ˆë‹¤. ê±´ë„ˆëœë‹ˆë‹¤: {next_url}")
        else:
            if next_url not in visited_urls:
                result = await download_file_with_aria2(
                    next_url, local_path, aria2_options, error_urls_file
                )
                if result:
                    visited_urls.add(next_url)
                    await save_visited_urls(visited_urls, visited_urls_file)
            else:
                print(f"â†ªï¸ ì´ë¯¸ ë‹¤ìš´ë¡œë“œí•œ íŒŒì¼ì…ë‹ˆë‹¤. ê±´ë„ˆëœë‹ˆë‹¤: {next_url}")


@app.command()
def main(
    base_url: str = typer.Option(
        "https://old-releases.ubuntu.com/releases/",
        "--base-url",
        "-u",
        help="ë‹¤ìš´ë¡œë“œí•  HTTP íŒŒì¼ì„œë²„ì˜ URL /ë¥¼ ë§¨ë’¤ì— ë¶™ì—¬ì£¼ì„¸ìš”.\n URLì˜ ê²½ë¡œê°€ íƒìƒ‰í•˜ëŠ” root ì§€ì ì´ ë©ë‹ˆë‹¤.",
    ),
    local_download_root: str = typer.Option(
        "/mnt/SSD-1TB/Mirror/ubuntu-old-releases",
        "--local-dir",
        "-d",
        help="íŒŒì¼ì´ ì €ì¥ë  ë¡œì»¬ ë””ë ‰í† ë¦¬ì˜ ì£¼ì†Œ",
    ),
    visited_urls_file: str = typer.Option(
        "./visited_urls.json",
        "--visited-log",
        help="ë°©ë¬¸í•œ íŒŒì¼ ê²½ë¡œì˜ URLì„ ì €ì¥í•  ë¡œê·¸íŒŒì¼ì˜ ì£¼ì†Œ",
    ),
    error_urls_file: str = typer.Option(
        "./error_urls.json",
        "--error-log",
        help="ì˜¤ë¥˜ê°€ ë°œìƒí•œ URLì„ ì €ì¥í•  ë¡œê·¸íŒŒì¼ì˜ ì£¼ì†Œ",
    ),
    aria2_options: Optional[List[str]] = typer.Option(
        ["-x", "16", "-s", "16", "-k", "1M", "--follow-torrent=false"],
        "--aria2-opts",
        help="aria2cì— ì „ë‹¬í•  ì¶”ê°€ ì˜µì…˜. ì˜ˆ: --aria2-opts -x --aria2-opts 10",
    ),
):
    asyncio.run(
        async_main(
            base_url,
            local_download_root,
            visited_urls_file,
            error_urls_file,
            aria2_options,
        )
    )


async def async_main(
    base_url: str,
    local_download_root: str,
    visited_urls_file: str,
    error_urls_file: str,
    aria2_options: Optional[List[str]],
):
    print("==============================================")
    print("  HTTP-Ssak3 (with aria2)  ")
    print("==============================================")
    print(f"ì„œë²„ URL: {base_url}")
    print(f"ì €ì¥ ê²½ë¡œ: {local_download_root}")
    print(f"ë°©ë¬¸ ê¸°ë¡ íŒŒì¼: {visited_urls_file}")
    print("----------------------------------------------")

    if not check_aria2c_installed():
        sys.exit(1)

    visited_urls = await load_visited_urls(visited_urls_file)

    target_base_url = base_url
    if not target_base_url.endswith("/"):
        target_base_url += "/"

    try:
        await sync_mirror(
            target_base_url,
            local_download_root,
            visited_urls,
            target_base_url,
            aria2_options,
            visited_urls_file,
            error_urls_file,
        )
        print("\nğŸ‰ ëª¨ë“  ì‘ì—…ì´ ì™„ë£Œë˜ì—ˆìŠµë‹ˆë‹¤.")
    except KeyboardInterrupt:
        print("\nğŸš« ì‚¬ìš©ìì— ì˜í•´ ì‘ì—…ì´ ì¤‘ë‹¨ë˜ì—ˆìŠµë‹ˆë‹¤.")
        print("   ë°©ë¬¸ ê¸°ë¡ì„ ì €ì¥í•˜ëŠ” ì¤‘...")
        await save_visited_urls(visited_urls, visited_urls_file)
        print("   ë‹¤ìŒì— ë‹¤ì‹œ ì‹¤í–‰í•˜ë©´ ì¤‘ë‹¨ëœ ì§€ì ë¶€í„° ì´ì–´ë°›ìŠµë‹ˆë‹¤.")
        sys.exit(0)
    except Exception as e:
        print(f"\nğŸš¨ ìŠ¤í¬ë¦½íŠ¸ ì‹¤í–‰ ì¤‘ ì˜ˆìƒì¹˜ ëª»í•œ ì˜¤ë¥˜ê°€ ë°œìƒí–ˆìŠµë‹ˆë‹¤: {e}")
        await save_visited_urls(visited_urls, visited_urls_file)
        sys.exit(1)
    finally:
        await save_visited_urls(visited_urls, visited_urls_file)
        print(f"ğŸ’¾ ë°©ë¬¸ ê¸°ë¡ì´ ì €ì¥ë˜ì—ˆìŠµë‹ˆë‹¤: {len(visited_urls)}ê°œ URL")


if __name__ == "__main__":
    app()
